{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b23f851",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from seqfacben.generators.random import RandomSequenceGenerator\n",
        "from seqfacben.tasks.sorting import SortingTask\n",
        "from seqfacben.models.simple_nn import SimpleNN\n",
        "from seqfacben.manager.task_manager import TaskManager\n",
        "from seqfacben.losses.sequence import cross_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41410d8",
      "metadata": {},
      "source": [
        "##### I. Single Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "983affb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Step 1000: train_loss=1.3151, train_acc=0.5298, val_loss=1.4312, val_acc=0.4863\n",
            "Step 2000: train_loss=0.9557, train_acc=0.6460, val_loss=0.9235, val_acc=0.6685\n",
            "Step 3000: train_loss=0.6927, train_acc=0.7471, val_loss=0.7392, val_acc=0.7329\n",
            "Step 4000: train_loss=0.5977, train_acc=0.7754, val_loss=0.8315, val_acc=0.7485\n",
            "Step 5000: train_loss=0.5894, train_acc=0.8174, val_loss=0.4823, val_acc=0.8335\n",
            "\n",
            "Testing:\n",
            "Final val_loss=0.4618, val_acc=0.8325\n",
            "\n",
            "Examples:\n",
            "\n",
            "Example 1:\n",
            "  Input:  [58, 58, 37, 0, 23, 1, 3, 40, 52, 63, 6, 48, 7, 17, 13, 21, 16, 22, 15, 31, 12, 40, 49, 24, 60, 13, 29, 47, 52, 27, 28, 27]\n",
            "  Target: [0, 1, 3, 6, 7, 12, 13, 13, 15, 16, 17, 21, 22, 23, 24, 27, 27, 28, 29, 31, 37, 40, 40, 47, 48, 49, 52, 52, 58, 58, 60, 63]\n",
            "  Pred:   [0, 1, 3, 6, 7, 12, 13, 13, 15, 15, 17, 22, 22, 23, 24, 27, 27, 28, 29, 31, 37, 40, 40, 47, 48, 49, 52, 52, 58, 58, 60, 63]\n",
            "  Correct: 30/32\n",
            "\n",
            "Example 2:\n",
            "  Input:  [2, 14, 21, 23, 52, 30, 15, 63, 6, 35, 1, 6, 58, 28, 63, 19, 12, 0, 24, 29, 21, 34, 48, 15, 27, 27, 59, 21, 46, 6, 57, 4]\n",
            "  Target: [0, 1, 2, 4, 6, 6, 6, 12, 14, 15, 15, 19, 21, 21, 21, 23, 24, 27, 27, 28, 29, 30, 34, 35, 46, 48, 52, 57, 58, 59, 63, 63]\n",
            "  Pred:   [0, 1, 2, 4, 6, 6, 6, 12, 15, 15, 15, 19, 21, 21, 21, 24, 23, 27, 27, 29, 30, 30, 34, 35, 46, 48, 52, 57, 58, 59, 63, 63]\n",
            "  Correct: 27/32\n",
            "\n",
            "Example 3:\n",
            "  Input:  [34, 45, 23, 26, 29, 33, 15, 40, 46, 34, 27, 44, 31, 54, 34, 57, 21, 8, 2, 25, 16, 17, 59, 37, 37, 19, 50, 30, 1, 1, 1, 41]\n",
            "  Target: [1, 1, 1, 2, 8, 15, 16, 17, 19, 21, 23, 25, 26, 27, 29, 30, 31, 33, 34, 34, 34, 37, 37, 40, 41, 44, 45, 46, 50, 54, 57, 59]\n",
            "  Pred:   [1, 1, 1, 2, 8, 15, 15, 17, 19, 21, 23, 23, 25, 27, 29, 30, 31, 34, 34, 34, 34, 34, 37, 40, 40, 44, 45, 46, 46, 54, 58, 59]\n",
            "  Correct: 24/32\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Generator parameters\n",
        "vocab_size = 64\n",
        "seq_len = 32\n",
        "\n",
        "# Model parameters\n",
        "d_model = 64\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "train_steps = 5000\n",
        "\n",
        "# Set up and train\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "generator = RandomSequenceGenerator(seq_len, vocab_size)\n",
        "task = SortingTask(generator, loss_fn=cross_entropy)\n",
        "model = SimpleNN(vocab_size, seq_len, d_model=d_model).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "manager = TaskManager(task, model, optimizer, device)\n",
        "\n",
        "# Train\n",
        "manager.train(n_steps=train_steps, batch_size=batch_size)\n",
        "\n",
        "# Test\n",
        "print(\"\\nTesting:\")\n",
        "val_loss, val_acc = manager.eval_step(batch_size=batch_size)\n",
        "print(f\"Final val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "manager.show_examples(n_examples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdb8dfa",
      "metadata": {},
      "source": [
        "##### II. Multi Experiment Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd5480c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Experiment configurations\n",
        "configs = [\n",
        "    {'vocab_size': 5, 'seq_len': 10},\n",
        "    {'vocab_size': 10, 'seq_len': 10},\n",
        "    {'vocab_size': 20, 'seq_len': 10},\n",
        "    {'vocab_size': 5, 'seq_len': 20},\n",
        "    {'vocab_size': 5, 'seq_len': 30},\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in configs:\n",
        "    vocab_size = config['vocab_size']\n",
        "    seq_len = config['seq_len']\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running: vocab_size={vocab_size}, seq_len={seq_len}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    generator = RandomSequenceGenerator(seq_len, vocab_size)\n",
        "    task = SortingTask(generator, loss_fn=cross_entropy)\n",
        "    model = SimpleNN(vocab_size, seq_len, d_model=64).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    manager = TaskManager(task, model, optimizer, device)\n",
        "    \n",
        "    # Train\n",
        "    history = manager.train(n_steps=10000, batch_size=32, eval_every=500)\n",
        "    \n",
        "    # Final evaluation\n",
        "    val_loss, val_acc = manager.eval_step(batch_size=256)\n",
        "    \n",
        "    # Save results\n",
        "    last = history[-1] if history else {}\n",
        "    results.append({\n",
        "        'vocab_size': vocab_size,\n",
        "        'seq_len': seq_len,\n",
        "        'final_train_loss': last.get('train_loss'),\n",
        "        'final_train_acc': last.get('train_acc'),\n",
        "        'final_val_loss': last.get('val_loss') or val_loss,\n",
        "        'final_val_acc': last.get('val_acc') or val_acc,\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nFinal val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(df)\n",
        "df.to_csv('sorting_experiments.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "seqfacben",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
